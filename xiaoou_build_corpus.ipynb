{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import scraper\n",
    "## save models, data...\n",
    "import stanza\n",
    "reload(scraper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='fr',processors='tokenize,mwt,lemma,pos,depparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_builder(label,text):\n",
    "    # run stanza\n",
    "    stanza_obj = nlp_stanza(text)\n",
    "    final_sentences = []\n",
    "    temp_sents = stanza_obj.sentences\n",
    "    with open(\"corpus.xml\",\"a+\") as f:\n",
    "        f.write(\"<art label=\\\"\"+label+\"\\\">\\n\")\n",
    "        for sent in temp_sents:\n",
    "            f.write(\"<s>\\n\")\n",
    "            temp_words = sent.words\n",
    "            # write word\n",
    "            for word in temp_words:\n",
    "                # if word.lemma.lower() not in myStopwords:\n",
    "                f.write(str(word.id)+\"\\t\"+word.text+\"\\t\"+word.lemma+\"\\t\"+word.upos+\"\\t\"+str(word.head)+\"\\t\"+word.deprel+\"\\n\") \n",
    "            # end sentence tag\n",
    "            f.write(\"</s>\\n\")\n",
    "        f.write(\"</art>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = scraper.decompress_pickle(\"corpus_complet_5themes.pbz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of 9200 articles for eco politique et societe\n",
    "df_sampled = df_complete[df_complete.label.isin(['politique','societe','economie'])]\n",
    "df_sampled = df_sampled.groupby('label').head(9200) # 27600 au total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "lemonde = open('corpus.xml', encoding='utf-8').read()\n",
    "# split articles\n",
    "lemonde_articles = re.split('</art>', lemonde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Build dataframe lists\n",
    "# all the articles\n",
    "articles_output = []\n",
    "# all the labels\n",
    "labels_output = []\n",
    "for article in lemonde_articles:\n",
    "    sentences = \"\"\n",
    "    # extract the label, exclude blank lines\n",
    "    if re.split(r\"</?s>\",article)[0] != \"\\n\":\n",
    "        label = re.split(r\"</?s>\",article)[0].split(\"\\\"\")[1]\n",
    "        labels_output.append(label)\n",
    "    for tokens in re.split(r\"</?s>\",article)[1:]:\n",
    "        if tokens != \"\\n\":\n",
    "            for token in tokens.split(\"\\n\"):\n",
    "                if token:\n",
    "                    # remove stop words\n",
    "                    if token.split(\"\\t\")[3].startswith(\"PROPN\") or token.split(\"\\t\")[3] in [\"NOUN\",\"VERB\",\"ADV\",\"ADJ\",\"SYM\",\"INTJ\"]:\n",
    "                        sentences += token.split(\"\\t\")[2] + \" \"\n",
    "        sentences += \"\\n\"\n",
    "    if sentences:\n",
    "        articles_output.append(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_total = scraper.build_df(labels_output,articles_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1000 = df_total.groupby('label').head(1000) # 3000 au total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_8200 = df_total.groupby('label').tail(8200) # 24600 au total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final1000.pickle\",\"wb\") as p:\n",
    "    pickle.dump(df_1000,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"final8200.pickle\",\"wb\") as p:\n",
    "    pickle.dump(df_8200,p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
