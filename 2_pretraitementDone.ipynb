{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stanza.download('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scraper\n",
    "## save models, data...\n",
    "import pickle\n",
    "## tokeniseur\n",
    "import stanza\n",
    "import spacy \n",
    "## stopwords\n",
    "from nltk.corpus import stopwords\n",
    "import chiffres # stopwords définis par nous\n",
    "import string\n",
    "## for data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "## for progress\n",
    "from tqdm import tqdm\n",
    "## for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "## for processing\n",
    "import re\n",
    "## for bag-of-words\n",
    "from sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing\n",
    "## for explainer\n",
    "from lime import lime_text\n",
    "## for word embedding\n",
    "import gensim\n",
    "import gensim.downloader as gensim_api\n",
    "## for deep learning\n",
    "from tensorflow.keras import models, layers, preprocessing as kprocessing\n",
    "from tensorflow.keras import backend as K\n",
    "## for bert language model\n",
    "# import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load('fr_core_news_md',disable = ['ner','parser'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords de nltk\n",
    "nltkStopWords = set(stopwords.words('french'))\n",
    "# nlp_spacy = spacy.load('fr_core_news_md')\n",
    "# des poncuations du package string avec «, »\n",
    "myPonct = { ponct for ponct in string.punctuation} | {\"«\",\"»\"}\n",
    "# le tout couplé avec les stopwords de spacy\n",
    "myStopwords = nltkStopWords | chiffres.customize_stopwords | nlp_spacy.Defaults.stop_words\\\n",
    "       | myPonct\n",
    "print(f'Les mots vides finaux sont au nombre de {len(myStopwords)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lire le corpus\n",
    "df = pd.read_csv('corpus_1000.csv')\n",
    "# filtrer les categories\n",
    "# df = df[df.label.isin(['culture','sport','economie'])]\n",
    "# 1000 articles par categorie\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 articles aleatoires\n",
    "df.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "fig.suptitle(\"nombre d'articles par catégorie\", fontsize=12)\n",
    "df[\"label\"].reset_index().groupby(\"label\").count().sort_values(by= \n",
    "       \"index\").plot(kind=\"barh\", legend=False, \n",
    "        ax=ax).grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_spacy = nlp_spacy(\"Emmanuel Macron est un bon politique qui vit à l'Avenue des Champs-Élysées.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test spacy, mot tiret séparé\n",
    "for word in doc_spacy:\n",
    "    print(word.text,end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_stanza = stanza.Pipeline(lang='fr',processors='tokenize,mwt,lemma,ner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_test = nlp_stanza(\"Emmanuel Macron est une bon politique qui vit à l'Avenue des Champs-Élysées. Jean de La Fontaine est un écrivain français.\")\n",
    "for token in doc_test.sentences[0].tokens:\n",
    "    print(token.text,end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entree : sentence object of stanza\n",
    "def tokenizer(text, flag_lemm=True, flag_lowercase=True, flag_rm_stop=True):\n",
    "    # run stanza\n",
    "    stanza_obj = nlp_stanza(text)\n",
    "    final_sentences = []\n",
    "    # keep the named entities together\n",
    "    for sent in stanza_obj.sentences:\n",
    "        debut = []\n",
    "        final_tokens = []\n",
    "        len_sent = len(sent.tokens) -1\n",
    "        for i,token in enumerate(sent.tokens):\n",
    "            if token.ner != \"O\":\n",
    "                debut.append(token.text)\n",
    "                if i == len_sent:\n",
    "                    final_tokens.append(\" \".join(debut))\n",
    "            else:\n",
    "                if debut:\n",
    "                    final_tokens.append(\" \".join(debut))\n",
    "                if flag_lemm: \n",
    "                    final_tokens.append(token.words[0].lemma)\n",
    "                else:\n",
    "                    final_tokens.append(token.words[0].text)\n",
    "                debut = []\n",
    "        # lowercase\n",
    "        if flag_lowercase:\n",
    "            final_tokens = [w.lower() for w in final_tokens]\n",
    "        # remove stopwords\n",
    "        if flag_rm_stop:\n",
    "            final_tokens = [w for w in final_tokens if w not in myStopwords]\n",
    "        ## back to string from list\n",
    "        final_text = \"|\".join(final_tokens)\n",
    "        # print(final_text)\n",
    "        final_sentences.append(final_text)\n",
    "        final_tokens = []\n",
    "    return \"||\".join(final_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_phrase = \"Emmanuel Macron est une bon politique qui vit à l'Avenue des Champs-Élysées. Jean de La Fontaine est un écrivain français.\"\n",
    "test_allflags = tokenizer(test_phrase)\n",
    "test_withstop = tokenizer(test_phrase,flag_rm_stop=False)\n",
    "test_allflags_nolemma = tokenizer(test_phrase,flag_lemm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_allflags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_allflags)\n",
    "print(test_withstop)\n",
    "print(test_allflags_nolemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text_clean\"] = df[\"text\"].apply(lambda x: \n",
    "          tokenizer(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"text_clean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to pickle\n",
    "with open(r\"corpus1000_5themes_cleaned.pickle\", \"wb\") as output_file:\n",
    "    pickle.dump(df, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complete = scraper.decompress_pickle(\"corpus_complet_5themes.pbz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df_complete.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a sample of 9000 articles for eco politique et societe\n",
    "df_sampled = df_complete[df_complete.label.isin(['politique','societe','economie'])]\n",
    "df_sampled = df_complete.groupby('label')['text'].apply(lambda s: s.sample(9000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sampled = df_sampled.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_des_phrases = []\n",
    "for text in df_sampled:\n",
    "    liste_des_phrases.append(tokenizer(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
